diff --git a/output/rocksdb/ee4b9966-5aa81f04-45ad75db/tools@db_stress.no_comments_mergebot.cc b/output/rocksdb/ee4b9966-5aa81f04-45ad75db/tools@db_stress.no_comments_truth.cc
index 364d899..5abbd91 100644
--- a/output/rocksdb/ee4b9966-5aa81f04-45ad75db/tools@db_stress.no_comments_mergebot.cc
+++ b/output/rocksdb/ee4b9966-5aa81f04-45ad75db/tools@db_stress.no_comments_truth.cc
@@ -13,0 +14 @@
+#include "rocksdb/statistics.h"
@@ -36,3 +37,84 @@ static bool ValidateUint32Range(const char* flagname, uint64_t value) {
-static const bool FLAGS_purge_redundant_percent_dummy __attribute__((unused)) =
-    google::RegisterFlagValidator(&FLAGS_purge_redundant_percent,
-                                  &ValidateInt32Percent);
+DEFINE_uint64(seed, 2341234, "Seed for PRNG");
+static const bool FLAGS_seed_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_seed, &ValidateUint32Range);
+DEFINE_int64(max_key, 1 * KB* KB,
+             "Max number of key/values to place in database");
+DEFINE_int32(column_families, 10, "Number of column families");
+DEFINE_bool(test_batches_snapshots, false,
+            "If set, the test uses MultiGet(), MultiPut() and MultiDelete()"
+            " which read/write/delete multiple keys in a batch. In this mode,"
+            " we do not verify db content by comparing the content with the "
+            "pre-allocated array. Instead, we do partial verification inside"
+            " MultiGet() by checking various values in a batch. Benefit of"
+            " this mode:\n"
+            "\t(a) No need to acquire mutexes during writes (less cache "
+            "flushes in multi-core leading to speed up)\n"
+            "\t(b) No long validation at the end (more speed up)\n"
+            "\t(c) Test snapshot and atomicity of batch writes");
+DEFINE_int32(threads, 32, "Number of concurrent threads to run.");
+DEFINE_int32(ttl, -1,
+             "Opens the db with this ttl value if this is not -1. "
+             "Carefully specify a large value such that verifications on "
+             "deleted values don't fail");
+DEFINE_int32(value_size_mult, 8,
+             "Size of value will be this number times rand_int(1,3) bytes");
+DEFINE_bool(verify_before_write, false, "Verify before write");
+DEFINE_bool(histogram, false, "Print histogram of operation timings");
+DEFINE_bool(destroy_db_initially, true,
+            "Destroys the database dir before start if this is true");
+DEFINE_bool (verbose, false, "Verbose");
+DEFINE_int32(write_buffer_size, rocksdb::Options().write_buffer_size,
+             "Number of bytes to buffer in memtable before compacting");
+DEFINE_int32(max_write_buffer_number,
+             rocksdb::Options().max_write_buffer_number,
+             "The number of in-memory memtables. "
+             "Each memtable is of size FLAGS_write_buffer_size.");
+DEFINE_int32(min_write_buffer_number_to_merge,
+             rocksdb::Options().min_write_buffer_number_to_merge,
+             "The minimum number of write buffers that will be merged together "
+             "before writing to storage. This is cheap because it is an "
+             "in-memory merge. If this feature is not enabled, then all these "
+             "write buffers are flushed to L0 as separate files and this "
+             "increases read amplification because a get request has to check "
+             "in all of these files. Also, an in-memory merge may result in "
+             "writing less data to storage if there are duplicate records in"
+             " each of these individual write buffers.");
+DEFINE_int32(open_files, rocksdb::Options().max_open_files,
+             "Maximum number of files to keep open at the same time "
+             "(use default if == 0)");
+DEFINE_int64(compressed_cache_size, -1,
+             "Number of bytes to use as a cache of compressed data."
+             " Negative means use default settings.");
+DEFINE_int32(compaction_style, rocksdb::Options().compaction_style, "");
+DEFINE_int32(level0_file_num_compaction_trigger,
+             rocksdb::Options().level0_file_num_compaction_trigger,
+             "Level0 compaction start trigger");
+DEFINE_int32(level0_slowdown_writes_trigger,
+             rocksdb::Options().level0_slowdown_writes_trigger,
+             "Number of files in level-0 that will slow down writes");
+DEFINE_int32(level0_stop_writes_trigger,
+             rocksdb::Options().level0_stop_writes_trigger,
+             "Number of files in level-0 that will trigger put stop.");
+DEFINE_int32(block_size, rocksdb::Options().block_size,
+             "Number of bytes in a block.");
+DEFINE_int32(max_background_compactions,
+             rocksdb::Options().max_background_compactions,
+             "The maximum number of concurrent background compactions "
+             "that can occur in parallel.");
+DEFINE_int32(max_background_flushes, rocksdb::Options().max_background_flushes,
+             "The maximum number of concurrent background flushes "
+             "that can occur in parallel.");
+DEFINE_int32(universal_size_ratio, 0, "The ratio of file sizes that trigger"
+             " compaction in universal style");
+DEFINE_int32(universal_min_merge_width, 0, "The minimum number of files to "
+             "compact in universal style compaction");
+DEFINE_int32(universal_max_merge_width, 0, "The max number of files to compact"
+             " in universal style compaction");
+DEFINE_int32(universal_max_size_amplification_percent, 0,
+             "The max size amplification for universal style compaction");
+DEFINE_int32(clear_column_family_one_in, 1000000,
+             "With a chance of 1/N, delete a column family and then recreate "
+             "it again. If N == 0, never drop/create column families. "
+             "When test_batches_snapshots is true, this flag has no effect");
+DEFINE_int64(cache_size, 2 * KB * KB * KB,
+             "Number of bytes to use as a cache of uncompressed data.");
@@ -46,0 +129,10 @@ static bool ValidateInt32Positive(const char* flagname, int32_t value) {
+DEFINE_int32(reopen, 10, "Number of times database reopens");
+static const bool FLAGS_reopen_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_reopen, &ValidateInt32Positive);
+DEFINE_int32(bloom_bits, 10, "Bloom filter bits per key. "
+             "Negative means use default settings.");
+DEFINE_string(db, "", "Use the db with the following name.");
+DEFINE_bool(verify_checksum, false,
+            "Verify checksum for every block read from storage");
+DEFINE_bool(mmap_read, rocksdb::EnvOptions().use_mmap_reads,
+            "Allow reads to occur via mmap-ing files");
@@ -47,0 +140,11 @@ static std::shared_ptr<rocksdb::Statistics> dbstats;
+DEFINE_bool(statistics, false, "Create database statistics");
+DEFINE_bool(sync, false, "Sync all writes to disk");
+DEFINE_bool(disable_data_sync, false,
+            "If true, do not wait until data is synced to disk.");
+DEFINE_bool(use_fsync, false, "If true, issue fsync instead of fdatasync");
+DEFINE_int32(kill_random_test, 0,
+             "If non-zero, kill at various points in source code with "
+             "probability 1/this");
+static const bool FLAGS_kill_random_test_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_kill_random_test,
+                                  &ValidateInt32Positive);
@@ -48,0 +152,8 @@ extern int rocksdb_kill_odds;
+DEFINE_bool(disable_wal, false, "If true, do not write WAL for write.");
+DEFINE_int32(target_file_size_base, 64 * KB,
+             "Target level-1 file size for compaction");
+DEFINE_int32(target_file_size_multiplier, 1,
+             "A multiplier to compute targe level-N file size (N >= 2)");
+DEFINE_uint64(max_bytes_for_level_base, 256 * KB, "Max bytes for level-1");
+DEFINE_int32(max_bytes_for_level_multiplier, 2,
+             "A multiplier to compute max bytes for level-N (N >= 2)");
@@ -56,0 +168,26 @@ static bool ValidateInt32Percent(const char* flagname, int32_t value) {
+DEFINE_int32(readpercent, 10,
+             "Ratio of reads to total workload (expressed as a percentage)");
+static const bool FLAGS_readpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_readpercent, &ValidateInt32Percent);
+DEFINE_int32(prefixpercent, 20,
+             "Ratio of prefix iterators to total workload (expressed as a"
+             " percentage)");
+static const bool FLAGS_prefixpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_prefixpercent, &ValidateInt32Percent);
+DEFINE_int32(writepercent, 45,
+             " Ratio of deletes to total workload (expressed as a percentage)");
+static const bool FLAGS_writepercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_writepercent, &ValidateInt32Percent);
+DEFINE_int32(delpercent, 15,
+             "Ratio of deletes to total workload (expressed as a percentage)");
+static const bool FLAGS_delpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_delpercent, &ValidateInt32Percent);
+DEFINE_int32(iterpercent, 10, "Ratio of iterations to total workload"
+             " (expressed as a percentage)");
+static const bool FLAGS_iterpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_iterpercent, &ValidateInt32Percent);
+DEFINE_uint64(num_iterations, 10, "Number of iterations per MultiIterate run");
+static const bool FLAGS_num_iterations_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_num_iterations, &ValidateUint32Range);
+DEFINE_bool(disable_seek_compaction, false,
+            "Option to disable compation triggered by read.");
@@ -73,0 +211,2 @@ enum rocksdb::CompressionType StringToCompressionType(const char* ctype) {
+DEFINE_string(compression_type, "snappy",
+              "Algorithm to use to compress the database");
@@ -75,0 +215 @@ static enum rocksdb::CompressionType FLAGS_compression_type_e =
+DEFINE_string(hdfs, "", "Name of hdfs environment");
@@ -76,0 +217,15 @@ static rocksdb::Env* FLAGS_env = rocksdb::Env::Default();
+DEFINE_uint64(ops_per_thread, 1200000, "Number of operations per thread.");
+static const bool FLAGS_ops_per_thread_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_ops_per_thread, &ValidateUint32Range);
+DEFINE_uint64(log2_keys_per_lock, 2, "Log2 of number of keys per lock");
+static const bool FLAGS_log2_keys_per_lock_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_log2_keys_per_lock,
+                                  &ValidateUint32Range);
+DEFINE_int32(purge_redundant_percent, 50,
+             "Percentage of times we want to purge redundant keys in memory "
+             "before flushing");
+static const bool FLAGS_purge_redundant_percent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_purge_redundant_percent,
+                                  &ValidateInt32Percent);
+DEFINE_bool(filter_deletes, false, "On true, deletes use KeyMayExist to drop"
+            " the delete if key not present");
@@ -78 +233,3 @@ enum RepFactory {
-kSkipList,kHashSkipList, kVectorRep
+  kSkipList,
+  kHashSkipList,
+  kVectorRep
@@ -91,0 +249 @@ static enum RepFactory FLAGS_rep_factory;
+DEFINE_string(memtablerep, "prefix_hash", "");
@@ -99,0 +258 @@ static bool ValidatePrefixSize(const char* flagname, int32_t value) {
+DEFINE_int32(prefix_size, 7, "Control the prefix size for HashSkipListRep");
@@ -101,0 +261,2 @@ static const bool FLAGS_prefix_size_dummy =
+DEFINE_bool(use_merge, false, "On true, replaces all writes with a Merge "
+            "that behaves like a Put");
@@ -250 +411,13 @@ public:
-  explicitSharedState(StressTest* stress_test): cv_(&mu_), seed_(FLAGS_seed), max_key_(FLAGS_max_key), log2_keys_per_lock_(FLAGS_log2_keys_per_lock), num_threads_(FLAGS_threads), num_initialized_(0), num_populated_(0), vote_reopen_(0), num_done_(0), start_(false), start_verify_(false), stress_test_(stress_test) {
+  explicit SharedState(StressTest* stress_test) :
+      cv_(&mu_),
+      seed_(FLAGS_seed),
+      max_key_(FLAGS_max_key),
+      log2_keys_per_lock_(FLAGS_log2_keys_per_lock),
+      num_threads_(FLAGS_threads),
+      num_initialized_(0),
+      num_populated_(0),
+      vote_reopen_(0),
+      num_done_(0),
+      start_(false),
+      start_verify_(false),
+      stress_test_(stress_test) {
@@ -366,250 +539,4 @@ Random rand;
-  ThreadState(uint32_t index, SharedState *shared): tid(index), rand(1000 + index + shared->GetSeed()), shared(shared) {
-  }
-};
-}
-namespace {
-class Stats {
-private:
-  double start_;
-  double finish_;
-  double seconds_;
-  long done_;
-  long gets_;
-  long prefixes_;
-  long writes_;
-  long deletes_;
-  long iterator_size_sums_;
-  long founds_;
-  long iterations_;
-  long errors_;
-  int next_report_;
-  size_t bytes_;
-  double last_op_finish_;
-  HistogramImpl hist_;
-public:
-  Stats(){ }
-  void Start() {
-    next_report_ = 100;
-    hist_.Clear();
-    done_ = 0;
-    gets_ = 0;
-    prefixes_ = 0;
-    writes_ = 0;
-    deletes_ = 0;
-    iterator_size_sums_ = 0;
-    founds_ = 0;
-    iterations_ = 0;
-    errors_ = 0;
-    bytes_ = 0;
-    seconds_ = 0;
-    start_ = FLAGS_env->NowMicros();
-    last_op_finish_ = start_;
-    finish_ = start_;
-  }
-  void Merge(const Stats& other) {
-    hist_.Merge(other.hist_);
-    done_ += other.done_;
-    gets_ += other.gets_;
-    prefixes_ += other.prefixes_;
-    writes_ += other.writes_;
-    deletes_ += other.deletes_;
-    iterator_size_sums_ += other.iterator_size_sums_;
-    founds_ += other.founds_;
-    iterations_ += other.iterations_;
-    errors_ += other.errors_;
-    bytes_ += other.bytes_;
-    seconds_ += other.seconds_;
-    if (other.start_ < start_) start_ = other.start_;
-    if (other.finish_ > finish_) finish_ = other.finish_;
-  }
-  void Stop() {
-    finish_ = FLAGS_env->NowMicros();
-    seconds_ = (finish_ - start_) * 1e-6;
-  }
-  void FinishedSingleOp() {
-    if (FLAGS_histogram) {
-      double now = FLAGS_env->NowMicros();
-      double micros = now - last_op_finish_;
-      hist_.Add(micros);
-      if (micros > 20000) {
-        fprintf(stdout, "long op: %.1f micros%30s\r", micros, "");
-      }
-      last_op_finish_ = now;
-    }
-    done_++;
-    if (done_ >= next_report_) {
-      if (next_report_ < 1000) next_report_ += 100;
-      else if (next_report_ < 5000) next_report_ += 500;
-      else if (next_report_ < 10000) next_report_ += 1000;
-      else if (next_report_ < 50000) next_report_ += 5000;
-      else if (next_report_ < 100000) next_report_ += 10000;
-      else if (next_report_ < 500000) next_report_ += 50000;
-      else next_report_ += 100000;
-      fprintf(stdout, "... finished %ld ops%30s\r", done_, "");
-    }
-  }
-  void AddBytesForWrites(int nwrites, size_t nbytes) {
-    writes_ += nwrites;
-    bytes_ += nbytes;
-  }
-  void AddGets(int ngets, int nfounds) {
-    founds_ += nfounds;
-    gets_ += ngets;
-  }
-  void AddPrefixes(int nprefixes, int count) {
-    prefixes_ += nprefixes;
-    iterator_size_sums_ += count;
-  }
-  void AddIterations(int n) {
-    iterations_ += n;
-  }
-  void AddDeletes(int n) {
-    deletes_ += n;
-  }
-  void AddErrors(int n) {
-    errors_ += n;
-  }
-  void Report(const char* name) {
-    std::string extra;
-    if (bytes_ < 1 || done_ < 1) {
-      fprintf(stderr, "No writes or ops?\n");
-      return;
-    }
-    double elapsed = (finish_ - start_) * 1e-6;
-    double bytes_mb = bytes_ / 1048576.0;
-    double rate = bytes_mb / elapsed;
-    double throughput = (double)done_/elapsed;
-    fprintf(stdout, "%-12s: ", name);
-    fprintf(stdout, "%.3f micros/op %ld ops/sec\n",
-            seconds_ * 1e6 / done_, (long)throughput);
-    fprintf(stdout, "%-12s: Wrote %.2f MB (%.2f MB/sec) (%ld%% of %ld ops)\n",
-            "", bytes_mb, rate, (100*writes_)/done_, done_);
-    fprintf(stdout, "%-12s: Wrote %ld times\n", "", writes_);
-    fprintf(stdout, "%-12s: Deleted %ld times\n", "", deletes_);
-    fprintf(stdout, "%-12s: %ld read and %ld found the key\n", "",
-            gets_, founds_);
-    fprintf(stdout, "%-12s: Prefix scanned %ld times\n", "", prefixes_);
-    fprintf(stdout, "%-12s: Iterator size sum is %ld\n", "",
-            iterator_size_sums_);
-    fprintf(stdout, "%-12s: Iterated %ld times\n", "", iterations_);
-    fprintf(stdout, "%-12s: Got errors %ld times\n", "", errors_);
-    if (FLAGS_histogram) {
-      fprintf(stdout, "Microseconds per op:\n%s\n", hist_.ToString().c_str());
-    }
-    fflush(stdout);
-  }
-};
-class SharedState {
-public:
-  static const uint32_t SENTINEL = 0xffffffff;
-  explicitSharedState(StressTest* stress_test): cv_(&mu_), seed_(FLAGS_seed), max_key_(FLAGS_max_key), log2_keys_per_lock_(FLAGS_log2_keys_per_lock), num_threads_(FLAGS_threads), num_initialized_(0), num_populated_(0), vote_reopen_(0), num_done_(0), start_(false), start_verify_(false), stress_test_(stress_test) {
-    if (FLAGS_test_batches_snapshots) {
-      key_locks_ = nullptr;
-      values_ = nullptr;
-      fprintf(stdout, "No lock creation because test_batches_snapshots set\n");
-      return;
-    }
-    values_ = new uint32_t[max_key_];
-    for (long i = 0; i < max_key_; i++) {
-      values_[i] = SENTINEL;
-    }
-    long num_locks = (max_key_ >> log2_keys_per_lock_);
-    if (max_key_ & ((1 << log2_keys_per_lock_) - 1)) {
-      num_locks ++;
-    }
-    fprintf(stdout, "Creating %ld locks\n", num_locks);
-    key_locks_ = new port::Mutex[num_locks];
-  }
-  ~SharedState() = delete;{
-    delete[] values_;
-    delete[] key_locks_;
-  }
-  port::Mutex* GetMutex() {
-    return &mu_;
-  }
-  port::CondVar* GetCondVar() {
-    return &cv_;
-  }
-  StressTest* GetStressTest() const {
-    return stress_test_;
-  }
-  long GetMaxKey() const {
-    return max_key_;
-  }
-  uint32_t GetNumThreads() const {
-    return num_threads_;
-  }
-  void IncInitialized() {
-    num_initialized_++;
-  }
-  void IncOperated() {
-    num_populated_++;
-  }
-  void IncDone() {
-    num_done_++;
-  }
-  void IncVotedReopen() {
-    vote_reopen_ = (vote_reopen_ + 1) % num_threads_;
-  }
-  bool AllInitialized() const {
-    return num_initialized_ >= num_threads_;
-  }
-  bool AllOperated() const {
-    return num_populated_ >= num_threads_;
-  }
-  bool AllDone() const {
-    return num_done_ >= num_threads_;
-  }
-  bool AllVotedReopen() {
-    return (vote_reopen_ == 0);
-  }
-  void SetStart() {
-    start_ = true;
-  }
-  void SetStartVerify() {
-    start_verify_ = true;
-  }
-  bool Started() const {
-    return start_;
-  }
-  bool VerifyStarted() const {
-    return start_verify_;
-  }
-  port::Mutex* GetMutexForKey(long key) {
-    return &key_locks_[key >> log2_keys_per_lock_];
-  }
-  void Put(long key, uint32_t value_base) {
-    values_[key] = value_base;
-  }
-  uint32_t Get(long key) const {
-    return values_[key];
-  }
-  void Delete(long key) const {
-    values_[key] = SENTINEL;
-  }
-  uint32_t GetSeed() const {
-    return seed_;
-  }
-private:
-  port::Mutex mu_;
-  port::CondVar cv_;
-  const uint32_t seed_;
-  const long max_key_;
-  const uint32_t log2_keys_per_lock_;
-  const int num_threads_;
-  long num_initialized_;
-  long num_populated_;
-  long vote_reopen_;
-  long num_done_;
-  bool start_;
-  bool start_verify_;
-  StressTest* stress_test_;
-  uint32_t *values_;
-  port::Mutex *key_locks_;
-};
-struct ThreadState {
-uint32_t tid;
-Random rand;
-  SharedState* shared;
-  Stats stats;
-  ThreadState(uint32_t index, SharedState *shared): tid(index), rand(1000 + index + shared->GetSeed()), shared(shared) {
+  ThreadState(uint32_t index, SharedState *shared)
+      : tid(index),
+        rand(1000 + index + shared->GetSeed()),
+        shared(shared) {
@@ -650,7 +576,0 @@ public:
-  }{
-    for (auto cf : column_families_) {
-      delete cf;
-    }
-    column_families_.clear();
-    delete db_;
-    delete filter_policy_;
@@ -793,0 +714,42 @@ private:
+  Status MultiGet(ThreadState* thread, const ReadOptions& readoptions,
+                  ColumnFamilyHandle* column_family, const Slice& key,
+                  std::string* value) {
+    std::string keys[10] = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9"};
+    Slice key_slices[10];
+    std::string values[10];
+    ReadOptions readoptionscopy = readoptions;
+    readoptionscopy.snapshot = db_->GetSnapshot();
+    Status s;
+    for (int i = 0; i < 10; i++) {
+      keys[i] += key.ToString();
+      key_slices[i] = keys[i];
+      s = db_->Get(readoptionscopy, column_family, key_slices[i], value);
+      if (!s.ok() && !s.IsNotFound()) {
+        fprintf(stderr, "get error: %s\n", s.ToString().c_str());
+        values[i] = "";
+        thread->stats.AddErrors(1);
+      } else if (s.IsNotFound()) {
+        values[i] = "";
+        thread->stats.AddGets(1, 0);
+      } else {
+        values[i] = *value;
+        char expected_prefix = (keys[i])[0];
+        char actual_prefix = (values[i])[0];
+        if (actual_prefix != expected_prefix) {
+          fprintf(stderr, "error expected prefix = %c actual = %c\n",
+                  expected_prefix, actual_prefix);
+        }
+        (values[i])[0] = ' ';
+        thread->stats.AddGets(1, 1);
+      }
+    }
+    db_->ReleaseSnapshot(readoptionscopy.snapshot);
+    for (int i = 1; i < 10; i++) {
+      if (values[i] != values[0]) {
+        fprintf(stderr, "error : inconsistent values for key %s: %s, %s\n",
+                key.ToString().c_str(), values[0].c_str(),
+                values[i].c_str());
+      }
+    }
+    return s;
+  }
@@ -851 +813,2 @@ private:
-  Status MultiIterate(ThreadState* thread, const ReadOptions& readoptions, ColumnFamilyHandle* column_family, const Slice& key) {
+  Status MultiIterate(ThreadState* thread, const ReadOptions& readoptions,
+                      ColumnFamilyHandle* column_family, const Slice& key) {
@@ -1023 +985,0 @@ private:
-<<<<<<< HEAD
@@ -1025,3 +986,0 @@ private:
-||||||| 45ad75db8
-=======
->>>>>>> 5aa81f04
@@ -1034 +993,2 @@ private:
-        if (i % (static_cast<int64_t>(1) << 8 * (8 - FLAGS_prefix_size)) == 0) {
+          if (i % (static_cast<int64_t>(1) << 8 * (8 - FLAGS_prefix_size)) ==
+              0) {
@@ -1063,0 +1024 @@ private:
+          VerifyValue(cf, i, options, shared, from_db, s, true);
@@ -1067 +1027,0 @@ private:
-          VerifyValue(cf, i, options, shared, from_db, s, true);
@@ -1077 +1037,3 @@ private:
-  void VerifyValue(int cf, long key, const ReadOptions& opts, const SharedState& shared, const std::string& value_from_db, Status s, bool strict = false) const {
+  void VerifyValue(int cf, long key, const ReadOptions& opts,
+                   const SharedState& shared, const std::string& value_from_db,
+                   Status s, bool strict = false) const {
@@ -1356,0 +1319 @@ private:
+ private:
@@ -1366,40 +1328,0 @@ private:
-  Status MultiGet(ThreadState* thread, const ReadOptions& readoptions, ColumnFamilyHandle* column_family, const Slice& key, std::string* value) {
-    std::string keys[10] = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9"};
-    Slice key_slices[10];
-    std::string values[10];
-    ReadOptions readoptionscopy = readoptions;
-    readoptionscopy.snapshot = db_->GetSnapshot();
-    Status s;
-    for (int i = 0; i < 10; i++) {
-      keys[i] += key.ToString();
-      key_slices[i] = keys[i];
-      s = db_->Get(readoptionscopy, column_family, key_slices[i], value);
-      if (!s.ok() && !s.IsNotFound()) {
-        fprintf(stderr, "get error: %s\n", s.ToString().c_str());
-        values[i] = "";
-        thread->stats.AddErrors(1);
-      } else if (s.IsNotFound()) {
-        values[i] = "";
-        thread->stats.AddGets(1, 0);
-      } else {
-        values[i] = *value;
-        char expected_prefix = (keys[i])[0];
-        char actual_prefix = (values[i])[0];
-        if (actual_prefix != expected_prefix) {
-          fprintf(stderr, "error expected prefix = %c actual = %c\n",
-                  expected_prefix, actual_prefix);
-        }
-        (values[i])[0] = ' ';
-        thread->stats.AddGets(1, 1);
-      }
-    }
-    db_->ReleaseSnapshot(readoptionscopy.snapshot);
-    for (int i = 1; i < 10; i++) {
-      if (values[i] != values[0]) {
-        fprintf(stderr, "error : inconsistent values for key %s: %s, %s\n",
-                key.ToString().c_str(), values[0].c_str(),
-                values[i].c_str());
-      }
-    }
-    return s;
-  }
