diff --git a/output/rocksdb/fb2346fc-45ad75db-d5de22dc/tools@db_stress.no_comments_mergebot.cc b/output/rocksdb/fb2346fc-45ad75db-d5de22dc/tools@db_stress.no_comments_truth.cc
index 1220e00..97da803 100644
--- a/output/rocksdb/fb2346fc-45ad75db-d5de22dc/tools@db_stress.no_comments_mergebot.cc
+++ b/output/rocksdb/fb2346fc-45ad75db-d5de22dc/tools@db_stress.no_comments_truth.cc
@@ -13,0 +14 @@
+#include "rocksdb/statistics.h"
@@ -36,2 +37,84 @@ static bool ValidateUint32Range(const char* flagname, uint64_t value) {
-static const bool FLAGS_prefix_size_dummy __attribute__((unused)) =
-    google::RegisterFlagValidator(&FLAGS_prefix_size, &ValidatePrefixSize);
+DEFINE_uint64(seed, 2341234, "Seed for PRNG");
+static const bool FLAGS_seed_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_seed, &ValidateUint32Range);
+DEFINE_int64(max_key, 1 * KB* KB,
+             "Max number of key/values to place in database");
+DEFINE_int32(column_families, 10, "Number of column families");
+DEFINE_bool(test_batches_snapshots, false,
+            "If set, the test uses MultiGet(), MultiPut() and MultiDelete()"
+            " which read/write/delete multiple keys in a batch. In this mode,"
+            " we do not verify db content by comparing the content with the "
+            "pre-allocated array. Instead, we do partial verification inside"
+            " MultiGet() by checking various values in a batch. Benefit of"
+            " this mode:\n"
+            "\t(a) No need to acquire mutexes during writes (less cache "
+            "flushes in multi-core leading to speed up)\n"
+            "\t(b) No long validation at the end (more speed up)\n"
+            "\t(c) Test snapshot and atomicity of batch writes");
+DEFINE_int32(threads, 32, "Number of concurrent threads to run.");
+DEFINE_int32(ttl, -1,
+             "Opens the db with this ttl value if this is not -1. "
+             "Carefully specify a large value such that verifications on "
+             "deleted values don't fail");
+DEFINE_int32(value_size_mult, 8,
+             "Size of value will be this number times rand_int(1,3) bytes");
+DEFINE_bool(verify_before_write, false, "Verify before write");
+DEFINE_bool(histogram, false, "Print histogram of operation timings");
+DEFINE_bool(destroy_db_initially, true,
+            "Destroys the database dir before start if this is true");
+DEFINE_bool (verbose, false, "Verbose");
+DEFINE_int32(write_buffer_size, rocksdb::Options().write_buffer_size,
+             "Number of bytes to buffer in memtable before compacting");
+DEFINE_int32(max_write_buffer_number,
+             rocksdb::Options().max_write_buffer_number,
+             "The number of in-memory memtables. "
+             "Each memtable is of size FLAGS_write_buffer_size.");
+DEFINE_int32(min_write_buffer_number_to_merge,
+             rocksdb::Options().min_write_buffer_number_to_merge,
+             "The minimum number of write buffers that will be merged together "
+             "before writing to storage. This is cheap because it is an "
+             "in-memory merge. If this feature is not enabled, then all these "
+             "write buffers are flushed to L0 as separate files and this "
+             "increases read amplification because a get request has to check "
+             "in all of these files. Also, an in-memory merge may result in "
+             "writing less data to storage if there are duplicate records in"
+             " each of these individual write buffers.");
+DEFINE_int32(open_files, rocksdb::Options().max_open_files,
+             "Maximum number of files to keep open at the same time "
+             "(use default if == 0)");
+DEFINE_int64(compressed_cache_size, -1,
+             "Number of bytes to use as a cache of compressed data."
+             " Negative means use default settings.");
+DEFINE_int32(compaction_style, rocksdb::Options().compaction_style, "");
+DEFINE_int32(level0_file_num_compaction_trigger,
+             rocksdb::Options().level0_file_num_compaction_trigger,
+             "Level0 compaction start trigger");
+DEFINE_int32(level0_slowdown_writes_trigger,
+             rocksdb::Options().level0_slowdown_writes_trigger,
+             "Number of files in level-0 that will slow down writes");
+DEFINE_int32(level0_stop_writes_trigger,
+             rocksdb::Options().level0_stop_writes_trigger,
+             "Number of files in level-0 that will trigger put stop.");
+DEFINE_int32(block_size, rocksdb::Options().block_size,
+             "Number of bytes in a block.");
+DEFINE_int32(max_background_compactions,
+             rocksdb::Options().max_background_compactions,
+             "The maximum number of concurrent background compactions "
+             "that can occur in parallel.");
+DEFINE_int32(max_background_flushes, rocksdb::Options().max_background_flushes,
+             "The maximum number of concurrent background flushes "
+             "that can occur in parallel.");
+DEFINE_int32(universal_size_ratio, 0, "The ratio of file sizes that trigger"
+             " compaction in universal style");
+DEFINE_int32(universal_min_merge_width, 0, "The minimum number of files to "
+             "compact in universal style compaction");
+DEFINE_int32(universal_max_merge_width, 0, "The max number of files to compact"
+             " in universal style compaction");
+DEFINE_int32(universal_max_size_amplification_percent, 0,
+             "The max size amplification for universal style compaction");
+DEFINE_int32(clear_column_family_one_in, 1000000,
+             "With a chance of 1/N, delete a column family and then recreate "
+             "it again. If N == 0, never drop/create column families. "
+             "When test_batches_snapshots is true, this flag has no effect");
+DEFINE_int64(cache_size, 2 * KB * KB * KB,
+             "Number of bytes to use as a cache of uncompressed data.");
@@ -45,0 +129,10 @@ static bool ValidateInt32Positive(const char* flagname, int32_t value) {
+DEFINE_int32(reopen, 10, "Number of times database reopens");
+static const bool FLAGS_reopen_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_reopen, &ValidateInt32Positive);
+DEFINE_int32(bloom_bits, 10, "Bloom filter bits per key. "
+             "Negative means use default settings.");
+DEFINE_string(db, "", "Use the db with the following name.");
+DEFINE_bool(verify_checksum, false,
+            "Verify checksum for every block read from storage");
+DEFINE_bool(mmap_read, rocksdb::EnvOptions().use_mmap_reads,
+            "Allow reads to occur via mmap-ing files");
@@ -46,0 +140,11 @@ static std::shared_ptr<rocksdb::Statistics> dbstats;
+DEFINE_bool(statistics, false, "Create database statistics");
+DEFINE_bool(sync, false, "Sync all writes to disk");
+DEFINE_bool(disable_data_sync, false,
+            "If true, do not wait until data is synced to disk.");
+DEFINE_bool(use_fsync, false, "If true, issue fsync instead of fdatasync");
+DEFINE_int32(kill_random_test, 0,
+             "If non-zero, kill at various points in source code with "
+             "probability 1/this");
+static const bool FLAGS_kill_random_test_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_kill_random_test,
+                                  &ValidateInt32Positive);
@@ -47,0 +152,8 @@ extern int rocksdb_kill_odds;
+DEFINE_bool(disable_wal, false, "If true, do not write WAL for write.");
+DEFINE_int32(target_file_size_base, 64 * KB,
+             "Target level-1 file size for compaction");
+DEFINE_int32(target_file_size_multiplier, 1,
+             "A multiplier to compute targe level-N file size (N >= 2)");
+DEFINE_uint64(max_bytes_for_level_base, 256 * KB, "Max bytes for level-1");
+DEFINE_int32(max_bytes_for_level_multiplier, 2,
+             "A multiplier to compute max bytes for level-N (N >= 2)");
@@ -55,0 +168,26 @@ static bool ValidateInt32Percent(const char* flagname, int32_t value) {
+DEFINE_int32(readpercent, 10,
+             "Ratio of reads to total workload (expressed as a percentage)");
+static const bool FLAGS_readpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_readpercent, &ValidateInt32Percent);
+DEFINE_int32(prefixpercent, 20,
+             "Ratio of prefix iterators to total workload (expressed as a"
+             " percentage)");
+static const bool FLAGS_prefixpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_prefixpercent, &ValidateInt32Percent);
+DEFINE_int32(writepercent, 45,
+             " Ratio of deletes to total workload (expressed as a percentage)");
+static const bool FLAGS_writepercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_writepercent, &ValidateInt32Percent);
+DEFINE_int32(delpercent, 15,
+             "Ratio of deletes to total workload (expressed as a percentage)");
+static const bool FLAGS_delpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_delpercent, &ValidateInt32Percent);
+DEFINE_int32(iterpercent, 10, "Ratio of iterations to total workload"
+             " (expressed as a percentage)");
+static const bool FLAGS_iterpercent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_iterpercent, &ValidateInt32Percent);
+DEFINE_uint64(num_iterations, 10, "Number of iterations per MultiIterate run");
+static const bool FLAGS_num_iterations_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_num_iterations, &ValidateUint32Range);
+DEFINE_bool(disable_seek_compaction, false,
+            "Option to disable compation triggered by read.");
@@ -72,0 +211,2 @@ enum rocksdb::CompressionType StringToCompressionType(const char* ctype) {
+DEFINE_string(compression_type, "snappy",
+              "Algorithm to use to compress the database");
@@ -74,0 +215 @@ static enum rocksdb::CompressionType FLAGS_compression_type_e =
+DEFINE_string(hdfs, "", "Name of hdfs environment");
@@ -75,0 +217,15 @@ static rocksdb::Env* FLAGS_env = rocksdb::Env::Default();
+DEFINE_uint64(ops_per_thread, 1200000, "Number of operations per thread.");
+static const bool FLAGS_ops_per_thread_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_ops_per_thread, &ValidateUint32Range);
+DEFINE_uint64(log2_keys_per_lock, 2, "Log2 of number of keys per lock");
+static const bool FLAGS_log2_keys_per_lock_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_log2_keys_per_lock,
+                                  &ValidateUint32Range);
+DEFINE_int32(purge_redundant_percent, 50,
+             "Percentage of times we want to purge redundant keys in memory "
+             "before flushing");
+static const bool FLAGS_purge_redundant_percent_dummy __attribute__((unused)) =
+    google::RegisterFlagValidator(&FLAGS_purge_redundant_percent,
+                                  &ValidateInt32Percent);
+DEFINE_bool(filter_deletes, false, "On true, deletes use KeyMayExist to drop"
+            " the delete if key not present");
@@ -77 +233,3 @@ enum RepFactory {
-kSkipList,kHashSkipList, kVectorRep
+  kSkipList,
+  kHashSkipList,
+  kVectorRep
@@ -90,0 +249 @@ static enum RepFactory FLAGS_rep_factory;
+DEFINE_string(memtablerep, "prefix_hash", "");
@@ -98,0 +258,5 @@ static bool ValidatePrefixSize(const char* flagname, int32_t value) {
+DEFINE_int32(prefix_size, 7, "Control the prefix size for HashSkipListRep");
+static const bool FLAGS_prefix_size_dummy =
+  google::RegisterFlagValidator(&FLAGS_prefix_size, &ValidatePrefixSize);
+DEFINE_bool(use_merge, false, "On true, replaces all writes with a Merge "
+            "that behaves like a Put");
@@ -332,0 +497,16 @@ public:
+  void LockColumnFamily(int cf) {
+    for (auto& mutex : key_locks_[cf]) {
+      mutex.Lock();
+    }
+  }
+  void UnlockColumnFamily(int cf) {
+    for (auto& mutex : key_locks_[cf]) {
+      mutex.Unlock();
+    }
+  }
+  void ClearColumnFamily(int cf) {
+    std::fill(values_[cf].begin(), values_[cf].end(), SENTINEL);
+  }
+  void Put(int cf, long key, uint32_t value_base) {
+    values_[cf][key] = value_base;
+  }
@@ -352,17 +531,0 @@ private:
-public:
-  void LockColumnFamily(int cf) {
-    for (auto& mutex : key_locks_[cf]) {
-      mutex.Lock();
-    }
-  }
-  void UnlockColumnFamily(int cf) {
-    for (auto& mutex : key_locks_[cf]) {
-      mutex.Unlock();
-    }
-  }
-  void ClearColumnFamily(int cf) {
-    std::fill(values_[cf].begin(), values_[cf].end(), SENTINEL);
-  }
-  void Put(int cf, long key, uint32_t value_base) {
-    values_[cf][key] = value_base;
-  }
@@ -414,7 +576,0 @@ public:
-  }{
-    for (auto cf : column_families_) {
-      delete cf;
-    }
-    column_families_.clear();
-    delete db_;
-    delete filter_policy_;
@@ -539 +695,64 @@ private:
-  Status MultiPrefixScan(ThreadState* thread, const ReadOptions& readoptions, ColumnFamilyHandle* column_family, const Slice& prefix, const Slice& key) {
+  Status MultiDelete(ThreadState* thread, const WriteOptions& writeoptions,
+                     ColumnFamilyHandle* column_family, const Slice& key) {
+    std::string keys[10] = {"9", "7", "5", "3", "1",
+                            "8", "6", "4", "2", "0"};
+    WriteBatch batch;
+    Status s;
+    for (int i = 0; i < 10; i++) {
+      keys[i] += key.ToString();
+      batch.Delete(column_family->GetID(), keys[i]);
+    }
+    s = db_->Write(writeoptions, &batch);
+    if (!s.ok()) {
+      fprintf(stderr, "multidelete error: %s\n", s.ToString().c_str());
+      thread->stats.AddErrors(1);
+    } else {
+      thread->stats.AddDeletes(10);
+    }
+    return s;
+  }
+  Status MultiGet(ThreadState* thread, const ReadOptions& readoptions,
+                  ColumnFamilyHandle* column_family, const Slice& key,
+                  std::string* value) {
+    std::string keys[10] = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9"};
+    Slice key_slices[10];
+    std::string values[10];
+    ReadOptions readoptionscopy = readoptions;
+    readoptionscopy.snapshot = db_->GetSnapshot();
+    Status s;
+    for (int i = 0; i < 10; i++) {
+      keys[i] += key.ToString();
+      key_slices[i] = keys[i];
+      s = db_->Get(readoptionscopy, column_family, key_slices[i], value);
+      if (!s.ok() && !s.IsNotFound()) {
+        fprintf(stderr, "get error: %s\n", s.ToString().c_str());
+        values[i] = "";
+        thread->stats.AddErrors(1);
+      } else if (s.IsNotFound()) {
+        values[i] = "";
+        thread->stats.AddGets(1, 0);
+      } else {
+        values[i] = *value;
+        char expected_prefix = (keys[i])[0];
+        char actual_prefix = (values[i])[0];
+        if (actual_prefix != expected_prefix) {
+          fprintf(stderr, "error expected prefix = %c actual = %c\n",
+                  expected_prefix, actual_prefix);
+        }
+        (values[i])[0] = ' ';
+        thread->stats.AddGets(1, 1);
+      }
+    }
+    db_->ReleaseSnapshot(readoptionscopy.snapshot);
+    for (int i = 1; i < 10; i++) {
+      if (values[i] != values[0]) {
+        fprintf(stderr, "error : inconsistent values for key %s: %s, %s\n",
+                key.ToString().c_str(), values[0].c_str(),
+                values[i].c_str());
+      }
+    }
+    return s;
+  }
+  Status MultiPrefixScan(ThreadState* thread, const ReadOptions& readoptions,
+                         ColumnFamilyHandle* column_family,
+                         const Slice& key) {
@@ -594 +813,2 @@ private:
-  Status MultiIterate(ThreadState* thread, const ReadOptions& readoptions, ColumnFamilyHandle* column_family, const Slice& key) {
+  Status MultiIterate(ThreadState* thread, const ReadOptions& readoptions,
+                      ColumnFamilyHandle* column_family, const Slice& key) {
@@ -598,0 +819 @@ private:
+    readoptionscopy.prefix_seek = FLAGS_prefix_size > 0;
@@ -699 +919,0 @@ private:
-<<<<<<< HEAD
@@ -701,6 +920,0 @@ private:
-          int count = 0;
-||||||| d5de22dc0
-          Iterator* iter = db_->NewIterator(read_opts);
-          int count = 0;
-=======
-          Iterator* iter = db_->NewIterator(read_opts);
@@ -708 +921,0 @@ private:
->>>>>>> 45ad75db
@@ -722,7 +935 @@ private:
-<<<<<<< HEAD
-          MultiPrefixScan(thread, read_opts, column_family, prefix);
-||||||| d5de22dc0
-          MultiPrefixScan(thread, read_opts, prefix);
-=======
-          MultiPrefixScan(thread, read_opts, key);
->>>>>>> 45ad75db
+          MultiPrefixScan(thread, read_opts, column_family, key);
@@ -1106,0 +1314 @@ private:
+ private:
@@ -1116,58 +1323,0 @@ private:
-  Status MultiDelete(ThreadState* thread, const WriteOptions& writeoptions, ColumnFamilyHandle* column_family, const Slice& key) {
-    std::string keys[10] = {"9", "7", "5", "3", "1",
-                            "8", "6", "4", "2", "0"};
-    WriteBatch batch;
-    Status s;
-    for (int i = 0; i < 10; i++) {
-      keys[i] += key.ToString();
-      batch.Delete(column_family->GetID(), keys[i]);
-    }
-    s = db_->Write(writeoptions, &batch);
-    if (!s.ok()) {
-      fprintf(stderr, "multidelete error: %s\n", s.ToString().c_str());
-      thread->stats.AddErrors(1);
-    } else {
-      thread->stats.AddDeletes(10);
-    }
-    return s;
-  }
-  Status MultiGet(ThreadState* thread, const ReadOptions& readoptions, ColumnFamilyHandle* column_family, const Slice& key, std::string* value) {
-    std::string keys[10] = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9"};
-    Slice key_slices[10];
-    std::string values[10];
-    ReadOptions readoptionscopy = readoptions;
-    readoptionscopy.snapshot = db_->GetSnapshot();
-    Status s;
-    for (int i = 0; i < 10; i++) {
-      keys[i] += key.ToString();
-      key_slices[i] = keys[i];
-      s = db_->Get(readoptionscopy, column_family, key_slices[i], value);
-      if (!s.ok() && !s.IsNotFound()) {
-        fprintf(stderr, "get error: %s\n", s.ToString().c_str());
-        values[i] = "";
-        thread->stats.AddErrors(1);
-      } else if (s.IsNotFound()) {
-        values[i] = "";
-        thread->stats.AddGets(1, 0);
-      } else {
-        values[i] = *value;
-        char expected_prefix = (keys[i])[0];
-        char actual_prefix = (values[i])[0];
-        if (actual_prefix != expected_prefix) {
-          fprintf(stderr, "error expected prefix = %c actual = %c\n",
-                  expected_prefix, actual_prefix);
-        }
-        (values[i])[0] = ' ';
-        thread->stats.AddGets(1, 1);
-      }
-    }
-    db_->ReleaseSnapshot(readoptionscopy.snapshot);
-    for (int i = 1; i < 10; i++) {
-      if (values[i] != values[0]) {
-        fprintf(stderr, "error : inconsistent values for key %s: %s, %s\n",
-                key.ToString().c_str(), values[0].c_str(),
-                values[i].c_str());
-      }
-    }
-    return s;
-  }
